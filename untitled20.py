# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tD6V9n81s67k9MzY6Xag9uSpNxjOCBWW
"""
from IPython.display import display
from matplotlib import pyplot as plt
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go
import seaborn as sns
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import joblib
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import make_pipeline
from statsmodels.tsa.arima.model import ARIMA

# Loading the dataset
df = pd.read_csv("predictive_maintenance.csv")
DataFrame = df ;

# Viewing the dataset
df.head(5)

#describing dataset
df.describe()

#Information
df.info()

# Selecting only numeric columns
numeric_columns = df.select_dtypes(include='number')
print(numeric_columns.head(5))

# Calculating the correlation matrix
corr_matrix = numeric_columns.corr()
print(corr_matrix)

# Setting up the matplotlib figure
fig, ax = plt.subplots(figsize=(10, 8))

# Creating the correlation heatmap using seaborn
#heatmap = sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True, ax=ax)
heatmap = sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='viridis', square=True, ax=ax)
#heatmap = sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='magma', square=True, ax=ax)
#heatmap = sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='cividis', square=True, ax=ax)

# Adding a title on the graph
plt.title('Correlation Heatmap')

# Adjusting the layout to prevent text cutting off the screen
plt.tight_layout()

# Saving the heatmap image with a higher DPI (dots per inch)
heatmap.get_figure().savefig('correlation_heatmap.png', bbox_inches='tight', dpi=300)

# Show the plot
plt.show()

# Checking pairplot as well for correlation
#sns.pairplot(corr_matrix)
#plt.show()

# Feature Distributions
# Visualizing the distribution of the numerical features in the dataset using histograms.

# List of variables
variables = ["Air temperature [K]", "Process temperature [K]", "Rotational speed [rpm]", "Torque [Nm]", "Tool wear [min]"]

# Create subplots with two columns
fig = make_subplots(rows=len(variables), cols=2, subplot_titles=[f'{var} Histogram and Boxplot' for var in variables])

for i, var in enumerate(variables, start=1):
    # Histogram
    histogram = px.histogram(df, x=var, nbins=20, title=f'{var} Histogram')
    histogram.update_layout(showlegend=False)

    # Boxplot
    boxplot = px.box(df, x=var, title=f'{var} Boxplot')
    boxplot.update_layout(showlegend=False)

    # Add the histograms and boxplots to the subplots
    fig.add_trace(histogram['data'][0], row=i, col=1)
    fig.add_trace(boxplot['data'][0], row=i, col=2)

    # Save each subplot as an individual image
    # histogram.write_image(f'{var}_histogram.png')
    # boxplot.write_image(f'{var}_boxplot.png')

# Update layout
fig.update_layout(height=400*len(variables), width=800, showlegend=False)

# Show the plot
fig.show()

# Machine Failure Analysis
# To analyze machine failures, we can create a bar chart to visualize the count of failures.
failure_counts = df["Failure Type"].value_counts()
fig = px.bar(failure_counts, x=failure_counts.index, y=failure_counts.values, labels={"x": "Failure Type", "y": "Count"})
fig.show()

# Descriptive Statistics
# Use the describe method to get summary statistics of the dataset
styled_data = df.describe().style\
.background_gradient(cmap='coolwarm')\
.set_properties(**{'text-align':'center','border':'1px solid black'})

# display styled data
display(styled_data)

#df.replace({'Failure Type':{'No Failure':0,'Heat Dissipation Failure':1,'Power Failure':2,'Overstrain Failure':3, 'Tool Wear Failure':4, 'Random Failure': 5}},inplace=True)
df.head()

import pandas as pd
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
import numpy as np

# Assuming df is your DataFrame and it contains the necessary columns

# Check unique values in 'Failure Type' before replacement
print("Unique values in 'Failure Type' before replacement:", df['Failure Type'].unique())

# Replace categorical labels with numerical labels
df.replace({'Failure Type': {'No Failure': 0, 'Heat Dissipation Failure': 1, 'Power Failure': 2, 'Overstrain Failure': 3, 'Tool Wear Failure': 4, 'Random Failure': 5, 'Random Failures': 5}}, inplace=True)

# Check for any non-numeric values in 'Failure Type'
print("Unique values in 'Failure Type' after replacement:", df['Failure Type'].unique())

# Checking for Null values in the dataset
print("Null values in each column:\n", df.isnull().sum())

# Dropping columns that are not needed for prediction
df = df.drop(['UDI', 'Product ID', 'Type','Target'], axis=1)

# Feature Engineering
df['Temperature Difference'] = df['Air temperature [K]'] - df['Process temperature [K]']
df['Power'] = df['Rotational speed [rpm]'] * df['Torque [Nm]']

# Define X and y (assuming 'Date' and 'Failure Type' are columns in your DataFrame)
X = df.drop(['Failure Type'], axis=1)
y = df['Failure Type']

print(df.tail(5))

# Ensure y is of integer type
y = y.astype(int)

# Check the shape of X and y
print("Shape of X:", X.shape)
print("Shape of y:", y.shape)
print("Unique classes in y:", np.unique(y))

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

# Split the training data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)

# Print counts of each class before SMOTE
print("Before OverSampling, counts of label '0': {}".format(sum(y_train == 0)))
print("Before OverSampling, counts of label '1': {}".format(sum(y_train == 1)))
print("Before OverSampling, counts of label '2': {}".format(sum(y_train == 2)))
print("Before OverSampling, counts of label '3': {}".format(sum(y_train == 3)))
print("Before OverSampling, counts of label '4': {}".format(sum(y_train == 4)))
print("Before OverSampling, counts of label '5': {}".format(sum(y_train == 5)))

# Apply SMOTE to the training data
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Print counts of each class after SMOTE
print("After OverSampling, counts of label '0': {}".format(sum(y_resampled == 0)))
print("After OverSampling, counts of label '1': {}".format(sum(y_resampled == 1)))
print("After OverSampling, counts of label '2': {}".format(sum(y_resampled == 2)))
print("After OverSampling, counts of label '3': {}".format(sum(y_resampled == 3)))
print("After OverSampling, counts of label '4': {}".format(sum(y_resampled == 4)))
print("After OverSampling, counts of label '5': {}".format(sum(y_resampled == 5)))

# Initializing the Decision Tree Classifier within a pipeline
model = make_pipeline(SMOTE(random_state=42), DecisionTreeClassifier(random_state=42))

# Training the model
model.fit(X_resampled, y_resampled)

# Make predictions on the test set
y_pred = model.predict(X_test)

# -----> Measuring the accuracy of the model

# Evaluate the model and print confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Save the trained model for future use
joblib.dump(model, 'tool_wear_prediction_model(DecisionTree).joblib')

X_train.head()

# Save the trained model for future use
joblib.dump(model, 'tool_wear_prediction_model1(DecisionTree).joblib')

# Using the newly created model to predict tool wear for new data
new_data = pd.DataFrame({
    'Air temperature [K]': [298.8],
    'Process temperature [K]': [308.9],
    'Rotational speed [rpm]': [1497],
    'Torque [Nm]': [50],
    'Tool wear [min]': [72],
    'Temperature Difference': [10],
    'Power': [1497* 50]
})

# Load the trained model
loaded_model = joblib.load('tool_wear_prediction_model1.joblib')

# Make predictions for the new data
prediction = loaded_model.predict(new_data)
print("Predicted Tool Wear:", prediction)

# Print interpretation of prediction
if prediction == 1:
    print("Tool is predicted to have Heat Dissipation Failure.")
elif prediction == 2:
    print("Tool is predicted to have Power Failure.")
elif prediction == 3:
    print("Tool is predicted to have Overstrain Failure.")
elif prediction == 4:
    print("Tool is predicted to have Tool Wear Failure.")
elif prediction == 5:
    print("Tool is predicted to have Random Failure.")
else:
    print("No failure predicted.")


y_pred_val= model.predict(X_val)
# -----> Measuring the accuracy of the model

# Evaluate the model and print confusion matrix
conf_matrix_val = confusion_matrix(y_val, y_pred_val)
print("Confusion Matrix:")
print(conf_matrix_val)

# Evaluate the model
print("Accuracy:", accuracy_score(y_val, y_pred_val))
print("\nClassification Report:\n", classification_report(y_val, y_val))

cols = X.columns
from sklearn.preprocessing import MinMaxScaler
ms = MinMaxScaler()
X1 = ms.fit_transform(X)
X1 = pd.DataFrame(X1, columns=[cols])
X1.head()

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=2, random_state=0)

kmeans.fit(X)

kmeans.cluster_centers_

kmeans.inertia_

labels = kmeans.labels_

# check how many of the samples were correctly labeled
correct_labels = sum(y == labels)

print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))

print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))

from sklearn.cluster import KMeans
cs = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)
    kmeans.fit(X)
    cs.append(kmeans.inertia_)
plt.plot(range(1, 11), cs)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('CS')
plt.show()

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=2,random_state=0)

kmeans.fit(X)

labels = kmeans.labels_

# check how many of the samples were correctly labeled

correct_labels = sum(y == labels)

print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))

print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))

kmeans = KMeans(n_clusters=3,random_state=0)

kmeans.fit(X)

labels = kmeans.labels_

# check how many of the samples were correctly labeled

correct_labels = sum(y == labels)

print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))

print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))

kmeans = KMeans(n_clusters=4,random_state=0)

kmeans.fit(X)

labels = kmeans.labels_

# check how many of the samples were correctly labeled

correct_labels = sum(y == labels)

print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))

print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))

kmeans = KMeans(n_clusters=5,random_state=0)

kmeans.fit(X)





labels = kmeans.labels_

# check how many of the samples were correctly labeled

correct_labels = sum(y == labels)

print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))

print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))

kmeans = KMeans(n_clusters=6,random_state=0)

kmeans.fit(X)

labels = kmeans.labels_

# check how many of the samples were correctly labeled

correct_labels = sum(y == labels)

print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))

print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

X2 = X;
y2 = y;


# Perform train-test split (considering temporal order if applicable)
X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)

# Apply SMOTE to the training data
smote = SMOTE(random_state=42)
X2_resampled, y2_resampled = smote.fit_resample(X2_train, y2_train)
# Initialize Random Forest Classifier
model_rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
model_rf.fit(X2_resampled, y2_resampled)

# Make predictions on the test set
y2_pred = model_rf.predict(X2_test)

# Calculate accuracy score
accuracy = accuracy_score(y2_test, y2_pred)
print(f"Accuracy Score : {accuracy:.2f}")

# Print classification report
print("\nClassification Report:")
print(classification_report(y2_test, y2_pred))

# Print confusion matrix
print("\nConfusion Matrix:")
print(confusion_matrix(y2_test, y2_pred))
joblib.dump(model, 'tool_wear_RandomForestprediction_model1.joblib')

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# Assuming X and y are your features and labels
X21_train, X21_val, y21_train, y21_val = train_test_split(X2_resampled, y2_resampled, test_size=0.2, random_state=42)

# Option 1: Using 'balanced'
model_balanced = RandomForestClassifier(class_weight='balanced', random_state=42)
model_balanced.fit(X21_train, y21_train)
y_pred_balanced = model_balanced.predict(X21_val)
print("Balanced Class Weight")
print(classification_report(y21_val, y_pred_balanced))
print(confusion_matrix(y21_val, y_pred_balanced))

# Option 2: Using 'balanced_subsample'
model_balanced_subsample = RandomForestClassifier(class_weight='balanced_subsample', random_state=42)
model_balanced_subsample.fit(X21_train, y21_train)
y_pred_balanced_subsample = model_balanced_subsample.predict(X21_val)
print("Balanced Subsample Class Weight")
print(classification_report(y21_val, y_pred_balanced_subsample))
print(confusion_matrix(y21_val, y_pred_balanced_subsample))

# Option 3: Manually specifying weights
class_weights = {0: 1, 1: 100, 2: 100, 3: 100, 4: 100, 5: 100}  # Example weights, adjust as needed
model_manual = RandomForestClassifier(class_weight=class_weights, random_state=42)
model_manual.fit(X21_train, y21_train)
y_pred_manual = model_manual.predict(X21_val)
print("Manual Class Weight")
print(classification_report(y21_val, y_pred_manual))
print(confusion_matrix(y21_val, y_pred_manual))

from statistics import mean, stdev
from sklearn import preprocessing
from sklearn.model_selection import StratifiedKFold
from sklearn import linear_model
from sklearn import datasets

skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)
lst_accu_stratified = []

for train_index, test_index in skf.split(X2, y2):
    x_train_fold, x_test_fold = X2_resampled.iloc[train_index], X2_resampled.iloc[test_index]
    y_train_fold, y_test_fold = y2.iloc[train_index], y2.iloc[test_index]
    model_rf.fit(x_train_fold, y_train_fold)
    lst_accu_stratified.append(model_rf.score(x_test_fold, y_test_fold))

# Print the output.
print('List of possible accuracy:', lst_accu_stratified)
print('\nMaximum Accuracy That can be obtained from this model is:',
      max(lst_accu_stratified)*100, '%')
print('\nMinimum Accuracy:',
      min(lst_accu_stratified)*100, '%')
print('\nOverall Accuracy:',
      mean(lst_accu_stratified)*100, '%')
print('\nStandard Deviation is:', stdev(lst_accu_stratified))



#TIME SERIES ANALYSIS

# Simulate a 'Date' column if not present
df['Date'] = pd.date_range(start='1900-01-01', periods=len(df), freq='1D')

# Set the Date column as the index
df.set_index('Date', inplace=True)

# Resample the data to 2-month intervals
df_resampled = df.resample('2ME').mean()

print(df.tail())

# Selecting the 'Failure Type' column for time series analysis
Failure_Type_series = df_resampled['Failure Type']

# Debug: print last few values of Failure_Type series
print("Last few values of Failure_Type series:")
print(Failure_Type_series.tail())

from statsmodels.tsa.stattools import adfuller
test_result=adfuller(df['Failure Type'])

def adfuller_test(sales):
    result=adfuller(sales)
    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']
    for value,label in zip(result,labels):
        print(label+' : '+str(value) )
    if result[1] <= 0.05:
        print("strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data has no unit root and is stationary")
    else:
        print("weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary ")
adfuller_test(df['Failure Type'])    


# Fit the ARIMA model
model_arima = ARIMA(Failure_Type_series, order=(5, 1, 0))
model_fit = model_arima.fit()

# Debug: print model summary
print("ARIMA model summary:")
print(model_fit.summary())

# Make forecast for the next 60 1 day intervals (2 months)
forecast = model_fit.get_forecast(steps=60)
forecast_values = forecast.predicted_mean
conf_int = forecast.conf_int()

# Create a DataFrame with the forecast results
forecast_dates = pd.date_range(start=Failure_Type_series.index[-1] + pd.Timedelta(days=1), periods=60, freq='1D')
forecast_df = pd.DataFrame({'Date': forecast_dates, 'Prediction': forecast_values})
forecast_df.set_index('Date', inplace=True)

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(Failure_Type_series, label='Historical Data')
plt.plot(forecast_df, label='Forecast', color='red')
plt.fill_between(forecast_df.index, conf_int.iloc[:, 0], conf_int.iloc[:, 1], color='pink', alpha=0.3)
plt.xlabel('Date')
plt.ylabel('Maintenance Target')
plt.title('Maintenance Prediction for the Next 2Months (60 1-Day Intervals)')
plt.legend()
plt.show()

# Debug: print forecast values and confidence intervals
print("Forecast values:")
print(forecast_df)
print("Confidence intervals:")
print(conf_int)

# Adjust the threshold for maintenance prediction
maintenance_threshold = forecast_df['Prediction'].mean() + 2 * forecast_df['Prediction'].std()  # New threshold

# Identify dates where maintenance is predicted to be needed
maintenance_dates = forecast_df[forecast_df['Prediction'] > maintenance_threshold].index

# Print the future maintenance dates
print("Future Maintenance Dates:")
print(maintenance_dates)

# Calculate the number of minutes until the next predicted maintenance
if not maintenance_dates.empty:
    months_until_next_maintenance = (maintenance_dates[0] - Failure_Type_series.index[-1]).total_seconds() / 43200
    print(f"Number of months until the next predicted maintenance: {months_until_next_maintenance:.2f} months")
else:
    print("No maintenance needed in the next 150 minutes.")

joblib.dump(model_fit, 'tool_wear_ARIMA_model2.joblib')



